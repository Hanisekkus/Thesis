
@online{link8,
  title = {Natural language processing},
  urldate = {2021-04-25},
  url = {https://en.wikipedia.org/wiki/Natural_language_processing},
}

@online{link9,
  title = {Zpracování přirozeného jazyka},
  urldate = {2021-04-25},
  url = {https://cs.wikipedia.org/wiki/Zpracov%C3%A1n%C3%AD_p%C5%99irozen%C3%A9ho_jazyka},
}

@online{link10,
  title = {5 Natural Language Processing Techniques for Extracting Information},
  author = {Neeraja Vaidya},
  urldate = {2021-04-25},
  url = {https://blog.aureusanalytics.com/blog/5-natural-language-processing-techniques-for-extracting-information},
}

@online{link11,
  title = {What Is Natural Language Processing and How Does It Work?},
  author = {Rachel Wolff},
  urldate = {2021-04-25},
  url = {https://monkeylearn.com/blog/what-is-natural-language-processing/},
}

@online{link12,
  title = {A Gentle Introduction to the Bag-of-Words Model},
  author = {Jason Brownlee PhD},
  urldate = {2021-04-25},
  url = {https://machinelearningmastery.com/gentle-introduction-bag-words-model/},
}

@online{link13,
  title = {Skip-Gram: NLP context words prediction algorithm},
  author = {Sanket Doshi},
  urldate = {2021-04-25},
  url = {https://towardsdatascience.com/skip-gram-nlp-context-words-prediction-algorithm-5bbf34f84e0c},
}

@online{link14,
  title = {An introduction to Bag of Words and how to code it in Python for NLP},
  author = {Praveen Dubey},
  urldate = {2021-04-25},
  url = {https://www.freecodecamp.org/news/an-introduction-to-bag-of-words-and-how-to-code-it-in-python-for-nlp-282e87a9da04/},
}


@online{link5,
  title = {Word embeddings},
  urldate = {2021-04-25},
  url = {https://www.tensorflow.org/tutorials/text/word_embeddings},
}

@online{link6,
  title = {A Beginner's Guide to Word2Vec and Neural Word Embeddings},
  urldate = {2021-04-25},
  url = {https://wiki.pathmind.com/word2vec},
}

@online{link7,
  title = {Word2Vec},
  urldate = {2021-04-25},
  url = {https://www.tensorflow.org/tutorials/text/word2vec},
}

@online{link1,
  title = {Umělá neuronová síť},
  urldate = {2021-04-25},
  url = {https://cs.wikipedia.org/wiki/Um%C4%9Bl%C3%A1_neuronov%C3%A1_s%C3%AD%C5%A54},
}

@online{link2,
  title = {RNN vs GRU vs LSTM},
  author = {Hemanth Pedamallu},
  urldate = {2021-04-25},
  url = {https://medium.com/analytics-vidhya/rnn-vs-gru-vs-lstm-863b0b7b1573},
}

@online{link3,
  title = {Understanding LSTM Networks},
  urldate = {2021-04-25},
  url = {http://colah.github.io/posts/2015-08-Understanding-LSTMs/},
}

@online{link4,
  title = {Understanding GRU Networks},
  author = {Simeon Kostadinov},
  urldate = {2021-04-25},
  url = {https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be},
}

@online{link15,
  title = {What are the main differences between the word embeddings of ELMo, BERT, Word2vec, and GloVe?},
  author = {Ajit Rajasekharan},
  urldate = {2021-04-25},
  url = {https://www.quora.com/What-are-the-main-differences-between-the-word-embeddings-of-ELMo-BERT-Word2vec-and-GloVe},
}

@online{link17,
  title = {How is GloVe different from word2vec?},
  author = {Stephan Gouws},
  urldate = {2021-04-25},
  url = {https://deeplearning.lipingyang.org/wp-content/uploads/2017/12/How-is-GloVe-different-from-word2vec_-Quora.pdf},
}

@online{link16,
  title = {How Recurrent Neural Networks work},
  author = {Simeon Kostadinov},
  urldate = {2021-04-25},
  url = {https://towardsdatascience.com/learn-how-recurrent-neural-networks-work-84e975feaaf7},
}

@inproceedings{link18,
    title = "{G}lo{V}e: Global Vectors for Word Representation",
    author = "Pennington, Jeffrey  and
      Socher, Richard  and
      Manning, Christopher",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1162",
    doi = "10.3115/v1/D14-1162",
    pages = "1532--1543",
}

@online{link19,
  title = {BERT Explained: State of the art language model for NLP},
  author = {Rani Horev},
  urldate = {2021-04-25},
  url = {https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270},
}

@online{link24,
  title = {Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processing},
  author = {Jacob Devlin, Ming-Wei Chang},
  urldate = {2022-05-28},
  url = {https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html},
}


@book{link20,
  author       = {Harrison Kinsley, Daniel Kukieła},
  title        = {Neural Networks from Scratch in Python},
  date         = 2020,
  keywords     = {primary},
  langid       = {english},
  langidopts   = {variant=american},
  shorttitle   = {Neural Networks},
}

@online{link21,
  title = {Training and plotting word2vec with bigrams},
  author = {Hamish},
  urldate = {2021-04-25},
  url = {https://www.kaggle.com/hamishdickson/training-and-plotting-word2vec-with-bigrams},
}

@online{link22,
  title = {An end-to-end open source machine learning platform},
  urldate = {2021-04-25},
  url = {https://www.tensorflow.org},
}

@online{link23,
  title = {Welcome To Colaboratory},
  urldate = {2021-04-29},
  url = {https://colab.research.google.com/},
}

@online{link25,
  title = {Transformer model for language understanding},
  urldate = {2022-05-29},
  url = {https://www.tensorflow.org/text/tutorials/transformer},
}

@online{link26,
  title = {Transformer Neural Network},
  urldate = {2022-05-26},
  url = {https://www.youtube.com/watch?v=TQQlZhbC5ps},
}

@inproceedings{link27,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@online{link28,
  title = {Transformer (machine learning model)},
  urldate = {2022-05-27},
  url = {https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)},
}

@inproceedings{link29,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@online{link30,
  title = {GPT-3},
  urldate = {2022-05-27},
  url = {https://en.wikipedia.org/wiki/GPT-3},
}

@online{link31,
  title = {Google Brain},
  urldate = {2022-05-27},
  url = {https://en.wikipedia.org/wiki/Google_Brain},
}

@online{link32,
  title = {IMDB},
  urldate = {2022-05-27},
  url = {https://www.imdb.com/},
}

@online{link33,
  title = {Google Colab},
  urldate = {2022-05-27},
  url = {https://colab.research.google.com/},
}

@online{link34,
  title = {Atention (machine learning)},
  urldate = {2022-05-29},
  url = {https://en.wikipedia.org/wiki/Attention_(machine_learning)},
}

@online{link35,
  title = {European Union Web},
  urldate = {2023-04-20},
  url = {https://european-union.europa.eu},
}

@online{link36,
  title = {Google Translate},
  urldate = {2023-04-20},
  url = {https://translate.google.com/},
}

@online{link37,
  title = {Deepl},
  urldate = {2023-04-20},
  url = {https://www.deepl.com},
}

@online{link38,
  title = {Python},
  urldate = {2023-04-20},
  url = {https://www.python.org/},
}

@online{link39,
  title = {Tensorflow},
  urldate = {2023-04-20},
  url = {https://www.tensorflow.org/},
}
